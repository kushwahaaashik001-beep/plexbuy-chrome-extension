name: üöÄ OPTIMA Premium Lead Scraper

on:
  # Har 30 minute mein (Production ready)
  schedule:
    - cron: '*/30 * * * *'
  
  # Manual run button
  workflow_dispatch:
    inputs:
      mode:
        description: 'Scraping Mode'
        required: false
        default: 'normal'
        type: choice
        options:
          - normal
          - aggressive
          - test

jobs:
  scrape-leads:
    name: "üí∞ OPTIMA Whale Hunting v3.2"
    runs-on: ubuntu-latest
    
    # Timeout: 15 minutes
    timeout-minutes: 15
    
    steps:
      # 1. Code Checkout
      - name: "üì¶ Checkout Repository"
        uses: actions/checkout@v3
        with:
          fetch-depth: 1
      
      # 2. Setup Python 3.10 (Important: 3.9 se better hai)
      - name: "üêç Setup Python 3.10"
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'  # Cache for faster installs
      
      # 3. UPGRADE PIP FIRST (MOST IMPORTANT STEP)
      - name: "‚¨ÜÔ∏è Upgrade pip and setuptools"
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip --version
      
      # 4. INSTALL DEPENDENCIES - SIMPLIFIED BUT COMPLETE
      - name: "üìö Install All Dependencies"
        run: |
          echo "Installing core dependencies..."
          pip install \
            duckduckgo-search \
            groq \
            supabase \
            requests \
            fake-useragent \
            beautifulsoup4 \
            lxml \
            aiohttp \
            backoff \
            python-dateutil \
            PyYAML \
            ujson \
            tenacity \
            ratelimit \
            tqdm \
            colorama
          
          # Verify key installations
          echo "‚úÖ Verifying installations..."
          python -c "import duckduckgo_search, groq, supabase, requests, aiohttp; print('All imports successful')"
      
      # 5. CREATE REQUIREMENTS.TXT IF NOT EXISTS
      - name: "üìù Create requirements.txt"
        run: |
          cat > requirements.txt << 'EOF'
          # OPTIMA Scraper - Complete Dependencies
          duckduckgo-search>=6.0.0
          groq>=0.9.0
          supabase>=2.3.0
          requests>=2.31.0
          fake-useragent>=1.4.0
          beautifulsoup4>=4.12.0
          lxml>=4.9.0
          aiohttp>=3.9.0
          backoff>=2.2.0
          python-dateutil>=2.8.0
          PyYAML>=6.0
          ujson>=5.8.0
          tenacity>=8.2.0
          ratelimit>=2.2.1
          tqdm>=4.66.0
          colorama>=0.4.6
          EOF
          echo "‚úÖ Created requirements.txt"
      
      # 6. FIND SCRAPER FILE (SMART SEARCH)
      - name: "üîç Find Scraper File"
        id: find-scraper
        run: |
          # Check multiple possible locations
          if [ -f "scraper.py" ]; then
            echo "Found: ./scraper.py"
            echo "SCRAPER_PATH=./scraper.py" >> $GITHUB_OUTPUT
          elif [ -f "backend-Optima/scraper.py" ]; then
            echo "Found: ./backend-Optima/scraper.py"
            echo "SCRAPER_PATH=./backend-Optima/scraper.py" >> $GITHUB_OUTPUT
          else
            # Search recursively
            SCRAPER_FILE=$(find . -name "scraper.py" -type f | head -1)
            if [ -n "$SCRAPER_FILE" ]; then
              echo "Found: $SCRAPER_FILE"
              echo "SCRAPER_PATH=$SCRAPER_FILE" >> $GITHUB_OUTPUT
            else
              echo "‚ùå ERROR: scraper.py not found!"
              echo "Listing Python files:"
              find . -name "*.py" -type f | head -10
              exit 1
            fi
          fi
      
      # 7. TEST IMPORTS BEFORE RUNNING
      - name: "üß™ Test Python Imports"
        run: |
          echo "Testing imports..."
          python -c "
          import sys
          sys.path.insert(0, '.')
          
          required_modules = [
              'duckduckgo_search',
              'groq', 
              'supabase',
              'requests',
              'fake_useragent',
              'bs4',
              'lxml',
              'aiohttp',
              'backoff',
              'dateutil',
              'yaml',
              'ujson'
          ]
          
          for module in required_modules:
              try:
                  __import__(module)
                  print(f'‚úÖ {module}')
              except ImportError as e:
                  print(f'‚ùå {module}: {e}')
                  sys.exit(1)
          
          print('üéâ All imports successful!')
          "
      
      # 8. VALIDATE SCRAPER CODE SYNTAX
      - name: "üìã Validate Scraper Syntax"
        run: |
          SCRAPER_PATH="${{ steps.find-scraper.outputs.SCRAPER_PATH }}"
          echo "Validating: $SCRAPER_PATH"
          
          # Check Python syntax
          python -m py_compile "$SCRAPER_PATH" && echo "‚úÖ Syntax OK"
          
          # Check for common errors
          echo "Checking for common issues..."
          if grep -q "match " "$SCRAPER_PATH" 2>/dev/null; then
            echo "‚ö†Ô∏è  Warning: 'match' statement found (requires Python 3.10+)"
          fi
      
      # 9. RUN THE SCRAPER WITH ENVIRONMENT VARIABLES
      - name: "üöÄ Run OPTIMA Scraper"
        env:
          # API KEYS (GitHub Secrets se)
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          
          # CONFIGURATION
          PYTHONUNBUFFERED: "1"
          TZ: "Asia/Kolkata"
          
        run: |
          SCRAPER_PATH="${{ steps.find-scraper.outputs.SCRAPER_PATH }}"
          
          echo "========================================"
          echo "üöÄ STARTING OPTIMA SCRAPER"
          echo "========================================"
          echo "üìÖ Time: $(date)"
          echo "üêç Python: $(python --version)"
          echo "üìÅ Scraper: $SCRAPER_PATH"
          echo "========================================"
          
          # Run with timeout and proper error handling
          timeout 600 python "$SCRAPER_PATH" 2>&1 | tee scraper.log
          
          SCRAPER_EXIT=$?
          
          echo "========================================"
          echo "üìä SCRAPER EXIT CODE: $SCRAPER_EXIT"
          echo "========================================"
          
          # Check for success/failure
          if [ $SCRAPER_EXIT -eq 0 ]; then
            echo "‚úÖ SUCCESS: Scraper completed"
            if grep -q "ENTERPRISE SCRAPING COMPLETE" scraper.log; then
              echo "üéâ Found completion message"
            fi
          elif [ $SCRAPER_EXIT -eq 124 ]; then
            echo "‚è∞ TIMEOUT: Scraper took too long (10 mins)"
          else
            echo "‚ùå FAILED: Scraper exited with code $SCRAPER_EXIT"
            echo "Last 20 lines of output:"
            tail -20 scraper.log
            exit $SCRAPER_EXIT
          fi
      
      # 10. UPLOAD LOGS FOR DEBUGGING
      - name: "üì§ Upload Logs"
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs
          path: |
            scraper.log
            optima_scraper.log
          retention-days: 7
      
      # 11. SUCCESS MESSAGE
      - name: "üéâ Success Notification"
        if: success()
        run: |
          echo "========================================"
          echo "üí∞ OPTIMA SCRAPER SUCCESSFUL!"
          echo "========================================"
          echo "‚úÖ Premium leads collected"
          echo "‚úÖ Database updated"
          echo "‚úÖ Check Supabase for results"
          echo "üïí Next run in 30 minutes"
          echo "========================================"
      
      # 12. FAILURE ANALYSIS
      - name: "üîç Failure Analysis"
        if: failure()
        run: |
          echo "========================================"
          echo "‚ùå OPTIMA SCRAPER FAILED"
          echo "========================================"
          echo "Common issues:"
          echo "1. Missing API keys in GitHub Secrets"
          echo "2. Python version mismatch (using 3.9 instead of 3.10)"
          echo "3. Dependency installation failed"
          echo "4. Network issues"
          echo ""
          echo "Debug steps:"
          echo "1. Check logs above"
          echo "2. Verify GitHub Secrets"
          echo "3. Test locally: pip install -r requirements.txt"
          echo "========================================"
